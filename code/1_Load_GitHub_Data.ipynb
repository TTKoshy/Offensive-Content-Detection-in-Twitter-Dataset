{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Twitter Dataset from GitHub\n",
        "This notebook was created to extract Tweets from Part 38 folder in GitHub to a csv file"
      ],
      "metadata": {
        "id": "h5zotU_dvwKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nu-TcoxmusSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb72f68-ef93-4fc5-e9f6-0c74db998cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcXcQ6T4uiGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3f7d0e-aac4-436a-85c6-8c6c09aab06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading chunk 21 for Part 38...\n",
            "Extracting chunk 21 for Part 38...\n",
            "Reading chunk 21 for Part 38...\n",
            "Downloading chunk 22 for Part 38...\n",
            "Extracting chunk 22 for Part 38...\n",
            "Reading chunk 22 for Part 38...\n",
            "Downloading chunk 23 for Part 38...\n",
            "Extracting chunk 23 for Part 38...\n",
            "Reading chunk 23 for Part 38...\n",
            "Downloading chunk 24 for Part 38...\n",
            "Extracting chunk 24 for Part 38...\n",
            "Reading chunk 24 for Part 38...\n",
            "Combined file saved: /content/drive/MyDrive/MRP_Offensive_Content_Detection/Twitter_Data/Part_38/Part_38_pre_processed.csv\n",
            "All parts processed successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Define directories\n",
        "data_url = \"https://github.com/sinking8/usc-x-24-us-election/tree/main\"\n",
        "base_folder = \"/content/drive/MyDrive/MRP_Offensive_Content_Detection/Twitter_Data/\"\n",
        "\n",
        "def process_data():\n",
        "\n",
        "    for part_number in range(38, 39):\n",
        "        part_folder = os.path.join(base_folder, f\"Part_{part_number}\")\n",
        "        os.makedirs(part_folder, exist_ok=True)\n",
        "\n",
        "        all_chunks = []\n",
        "        chunk_start = 21      # chunk_start\n",
        "        chunk_end = 24        # chunk_end\n",
        "\n",
        "        for i in range(chunk_start, chunk_end + 1):\n",
        "            file_url = f\"https://github.com/sinking8/usc-x-24-us-election/raw/main/part_{part_number}/october_chunk_{i}.csv.gz\"\n",
        "            compressed_file = os.path.join(part_folder, f\"october_chunk_{i}.csv.gz\")\n",
        "            extracted_file = os.path.join(part_folder, f\"october_chunk_{i}.csv\")\n",
        "\n",
        "            # Download the file\n",
        "            print(f\"Downloading chunk {i} for Part {part_number}...\")\n",
        "            os.system(f\"wget -q {file_url} -O {compressed_file}\")\n",
        "\n",
        "            # Extract the .gz file\n",
        "            print(f\"Extracting chunk {i} for Part {part_number}...\")\n",
        "            try:\n",
        "                with gzip.open(compressed_file, 'rb') as f_in:\n",
        "                    with open(extracted_file, 'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "                # Read the extracted CSV file\n",
        "                print(f\"Reading chunk {i} for Part {part_number}...\")\n",
        "                df = pd.read_csv(extracted_file)\n",
        "\n",
        "                # Convert epoch to date\n",
        "                if \"epoch\" in df.columns:\n",
        "                    df[\"date\"] = df[\"epoch\"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))\n",
        "\n",
        "                all_chunks.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing chunk {i} for Part {part_number}: {e}\")\n",
        "\n",
        "        # Combine all chunks into one DataFrame\n",
        "        if all_chunks:\n",
        "            combined_df = pd.concat(all_chunks, ignore_index=True)\n",
        "            processed_file = os.path.join(part_folder, f\"Part_{part_number}_pre_processed.csv\")\n",
        "\n",
        "            combined_df.to_csv(processed_file, index=False)\n",
        "            print(f\"Combined file saved: {processed_file}\")\n",
        "\n",
        "# Execute the processing\n",
        "if __name__ == \"__main__\":\n",
        "    process_data()\n",
        "    print(\"All parts processed successfully!\")\n"
      ]
    }
  ]
}